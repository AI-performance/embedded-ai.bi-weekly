---
layout: default
---

# 嵌入式AI简报 (2019-12-02)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  
<font>注：PC端微信链接打不开请用手机打开</font>


## 业界新闻  


- [谷歌发布人体图像分割工具BodyPix 2.0，支持在浏览器上多人识别，可在iPhone上流畅运行 | 量子位](https://mp.weixin.qq.com/s/s8thWGNdkYwKl7eQVIlJPw)  
摘要：Google官方推出了使用TensorFlow.js的人体图像分割工具BodyPix2.0，加入多人支持，并提高准确率。BodyPix能将图像分割为属于人的和不属于人的像素，属于人的部分可以进一步分类为二十四个身体部位之一。可直接在浏览器中运行，开发者只需加入几行代码，即可与电脑或手机的摄像头配合使用。  
在使用默认设置的情况下，BodyPix可以在15寸MacBook Pro上以25 fps的速度估计，并渲染人和身体部位的分割。而在iPhone X上，BodyPix能以21 fps的速度进行估计。  
试玩Demo：https://storage.googleapis.com/tfjs-models/demos/body-pix/index.html  
官方博客：https://blog.tensorflow.org/2019/11/updated-bodypix-2.html  
GitHub地址：https://github.com/tensorflow/tfjs-models/tree/master/body-pix  
- [联发科发布史上跑分最强5G SoC芯片——天玑1000！，超华为麒麟990 | 芯东西](https://mp.weixin.qq.com/s/IKvjqv9u1SPDcgcJ_i3i-A)  
摘要：芯片大厂联发科宣布推出5G芯片新品牌天玑，名源于北斗七星之一，其意为领先，并推出该品牌首款产品5G SoC芯片——天玑1000。天玑1000采用7nm工艺制造，基于此前联发科推出的多模5G Modem M70打造，5G网度最快在Sub-6GHz下行可达4.7Gbps，上行2.5Gbps。据称，这也是目前市面上推出的5G芯片中网速最快的芯片。  
CPU基于Arm最新的Cortex-A77核，单核频率2.6GHz，采用四合一架构。GPU采用Arm最新的Mali-G77核，采用九合一架构，比上一代G76运算速率提升40%。AI独立处理单元APU 3.0，并首次引入大小核概念，采用2大核+3小核+1微小核的架构。在运算能力上，特别针对浮点运算优化。  
- [小米开发者大会：MACE 进化, 小爱同学 3.0 问世，IoT 进军 B 端 | 雷锋网](https://mp.weixin.qq.com/s/zgAhELILW7HdFvZWLpEc9w)  
摘要：2019 年 11 月 19 日，小米开发者大会（MIDC，MI Developer Conference）在北京举行，这是小米继去年的 MIDC 开发者大会之后举行的第二届开发者大会。本次大会正值 5G 商用落地不久，因而有着不一样的意义。  
小米移动端深度学习框架 MACE。它是在 2018 年 6 月开源的，开源以后获得了广泛好评；目前，它的调用次数已经超过 5000 万次。  
在现场，崔宝秋发布了升级后的 MACE 0.12.0，它能够支持更丰富的异构计算算子、降级跨设备运行性能损失，新增 Kaldi 语音识别算子支持，新增了 CMake 支持。
崔宝秋表示，MACE 未来将发布更多的功能。其中，MACE-Kit 即将开源，未来还将支持更多的微控制器，全面支持手机和 IoT 设备超低功耗推理场景。
谈到 NLP 技术，在公布小米的 MiNLP 平台调用次数每天超过 60 亿之后，崔宝秋正式发布了 MiNLP 1.2 版本，MiNLP 1.2 显然有不少功能点，其中最重要的是从词法分析扩展到句法和语义分析。  
同时，崔宝秋也宣布小米分布式 KV 存储系统 Pegasus 1.12 版本上线，目前已经可以在 GitHub 下载。  
在现场，崔宝秋宣布，小爱同学 3.0 正式上线，用户动动嘴就能够升级。小爱同学 3.0 是首个在智能手机上实现自然连续对话的语音助理。同时，它将拥有更加自然甜美的女声，并且新增了男声版本。  
- [OPPO自研芯片曝光 M1处理器已注册商标 | 安兔兔](https://mp.weixin.qq.com/s/HBFiTjxV7Ttkab22BV8zZA)  
摘要：越来越多的手机厂家开始进行或自主研发（如小米澎湃系列）或寻求合作（vivo和三星）的路径，以备自己的不时之需，国产厂商OPPO最近也开始有所举动了。据网络消息，OPPO已经招募了多名Speadtrum和联发科的工程师，负责生产OPPO自主研发的移动芯片系列，首款芯片或被命名为”OPPO M1“。目前“OPPO M1”商标已经通过了欧盟知识产权局（EUIPO）的批准。  
OPPO M1的商标说明包括“芯片[集成电路]；半导体芯片；电脑芯片；多处理器芯片；用于集成电路制造的电子芯片；生物芯片；智能手机;手机；屏幕。”这清楚地表明它确实是智能手机芯片组。而且鉴于OPPO对于5G网络的布局和承诺，该自研芯片可能会支持5G网络。  
- [Win10 Mobile退场的余波，微软小娜告别移动端 | 安兔兔](https://mp.weixin.qq.com/s/mBFsss4F89DRzgKxV5JCEQ)  
摘要：照常理来说，既然长期带着“智障”帽子的Siri都能活得好好的，微软小娜自然也没理由退出。没错，微软小娜折戟移动端显然并不是微软在AI上“技不如人，甘拜下风”，恰恰相反，小娜的退出完全与技术无关，其只是微软在移动端上全面收缩的一个缩影。  
为什么“智障”的Siri能够有被逐步完善的机会，是因为苹果iPhone自带的智能语音助手就是她，iPhone用户长按HOME键就只能呼出Siri，这也为被吐槽的Siri能拥有超过5亿活跃用户的“秘籍”。同理，亚马逊Alexa能够日渐壮大，是因为有上亿销量的智能音箱Echo，而Google Assistant的层出不穷的黑科技，则是因为有十位数的Android设备作为后盾。  
反观微软小娜为什么会退出iOS以及Android端，究其原因则还是因为自家的Windows 10 Mobile一直不给力，以至于在去年微软确定自家的移动操作系统因为硬件缺乏支持，以及软件丰富程度远逊竞争对手的情况下，已经进入了“维护期”。自然就又双叒叕一次Windows 10 Mobile面临着“先有鸡还是先有蛋”的问题，也就是没有用户就吸引不到第三方开发者，没有第三方开发者贡献的内容，用户体验就无法保证。  


## 论文

- [1909.09757] [Positive-Unlabeled Compression on the Cloud | Yixing Xu、Yunhe Wang、Hanting Chen、Kai Han、Chunjing Xu、Dacheng Tao、Chang Xu](https://arxiv.org/pdf/1909.09757.pdf)  
摘要：卷积神经网络被广泛应用于诸多 CV 领域的任务中。为了保证性能，神经网络会存在大量的冗余参数。为了将 神经网络应用于移动设备中，往往需要压缩网络。但是上传模型到云端进行压缩时，耗时较长。为了解决这个问题，本文提出了一种基于少样本的云端网络压缩技术，实验表明，论文中的算法只需要依赖少量原始训练数据，达到和使用全部样本的压缩算法相当的准确率。  
- [1911.08609] [Hybrid Composition with IdleBlock: More Efficient Networks for Image Recognition | Bing Xu、Andrew Tulloch、Yunpeng Chen、Xiaomeng Yang、Lin Qiao](https://arxiv.org/pdf/1911.08609.pdf)  
摘要：近年来，卷积神经网络（CNN）已经主宰了计算机视觉领域。自 AlexNet 诞生以来，计算机视觉社区已经找到了一些能够改进 CNN 的设计，让这种骨干网络变得更加强大和高效，其中比较出色的单个分支网络包括 Network in Network、VGGNet、ResNet、DenseNet、ResNext、MobileNet v1/v2/v3 和 ShuffleNet v1/v2。近年来同样吸引了研究社区关注的还有多分辨率骨干网络。  
作者认为目前实现高效卷积网络的工作流程可以分成两步：1）设计一种网络架构；2）对该网络中的连接进行剪枝。在第一步，作者研究了人类专家设计的架构与搜索得到的架构之间的共同模式：对于每种骨干网络，其架构都是由其普通模块和归约模块（reduction block）的设计所确定的。第二步会将某些连接剪枝去掉，这样就不能保证每个模块都有完整的信息交换了。Facebook AI 的研究者在这篇论文中通过在网络设计步骤中考虑剪枝，为图像识别任务设计了一种更高效的网络。他们创造了一种新的模块设计方法：Idle。  
- [1911.09723v1] [Fast Sparse ConvNets | Erich Elsen、Marat Dukhan、Trevor Gale、Karen Simonyan](https://arxiv.org/abs/1911.09723v1)  
摘要：从历史发展的角度来看，对有效推理（efficient inference）的追求已经成为研究新的深度学习架构和构建块背后的驱动力之一。近来的一些示例包括：压缩和激发模块（squeeze-and-excitation module）、Xception 中的深度级可分离卷积（depthwise seperable convolution）和 MobileNet v2 中的倒置瓶颈（inverted bottleneck）。在所有这些示例中，生成的构建块不仅实现了更高的有效性和准确率，而且在领域内得到广泛采用。在本文中，来自 DeepMind 和 Google 的研究者们进一步扩展了神经网络架构的有效构建块，并且在没有结合标准基本体（standard primitive）的情况下，他们主张用稀疏对应（sparse counterpart）来替换这些密集基本体（dense primitive）。利用稀疏性来减少参数数量的想法并不新鲜，传统观点也认为理论浮点运算次数的减少不能转化为现实世界的效率增益。  
研究者通过提出一类用于 ARM 和 WebAssembly 的有效稀疏核来纠正这种错误观点，并且进行开源作为 XNNPACK 库的组成部分。借助于稀疏标准体（sparse primitive）的有效实现，研究者表明，MobileNet v1、MobileNet v2 和 EfficientNet 架构的稀疏版本在有效性和准确率曲线（efficiency-accuracy curve）上显著优于强大的密集基线（dense baseline）。在骁龙 835 芯片上，他们提出的稀疏网络比同等的密集网络性能增强 1.3-2.4 倍，这几乎相当于 MobileNet-family 一整代的性能提升。研究者希望他们的研究成果可以促进稀疏性更广泛地用作创建有效和准确深度学习架构的工具。
 
 
## 开源项目

- [google/XNNPACK: High-efficiency floating-point neural network inference operators for mobile and Web](https://github.com/google/XNNPACK)  
摘要：XNNPACK is a highly optimized library of floating-point neural network inference operators for ARM, WebAssembly, and x86 (SSE2 level) platforms. XNNPACK is not intended for direct use by deep learning practitioners and researchers; instead it provides low-level performance primitives for accelerating high-level machine learning frameworks, such as MediaPipe, TensorFlow Lite, and TensorFlow.js.  
- [rbonghi/jetson_stats: 📊 Simple package to monitoring and control your NVIDIA Jetson](https://github.com/rbonghi/jetson_stats)  
摘要：jetson-stats is a package to monitoring and control your NVIDIA Jetson [Nano, Xavier, TX2i, TX2, TX1] embedded board.  
- [pytorch/FBGEMM: FB (Facebook) + GEMM (General Matrix-Matrix Multiplication) - https://code.fb.com/ml-applications/fbgemm/](https://github.com/pytorch/FBGEMM)  
摘要：FBGEMM (Facebook GEneral Matrix Multiplication) is a low-precision, high-performance matrix-matrix multiplications and convolution library for server-side inference.  
The library provides efficient low-precision general matrix multiplication for small batch sizes and support for accuracy-loss minimizing techniques such as row-wise quantization and outlier-aware quantization. FBGEMM also exploits fusion opportunities in order to overcome the unique challenges of matrix multiplication at lower precision with bandwidth-bound operations.  
FBGEMM is used as a backend of Caffe2 and PyTorch quantized operators for x86 machines:  
  1. Caffe2: https://github.com/pytorch/pytorch/tree/master/caffe2/quantization/server  
  2. PyTorch: https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/quantized/cpu  
https://engineering.fb.com/ml-applications/fbgemm/  
- [DynamoRIO/drmemory: Memory Debugger for Windows, Linux, Mac, and Android](https://github.com/DynamoRIO/drmemory)  
摘要：Dr. Memory is a memory monitoring tool capable of identifying memory-related programming errors such as accesses of uninitialized memory, accesses to unaddressable memory (including outside of allocated heap units and heap underflow and overflow), accesses to freed memory, double frees, memory leaks, and (on Windows) handle leaks, GDI API usage errors, and accesses to un-reserved thread local storage slots.  
Dr. Memory operates on unmodified application binaries running on Windows, Linux, Mac, or Android on commodity IA-32, AMD64, and ARM hardware.


## 博文

- [微软小冰是怎样学会对话、唱歌和比喻？我们听三位首席科学家讲了讲背后的原理 | 量子位](https://mp.weixin.qq.com/s/q7YpDssTcMLZrhV_JIikpg)  
摘要：“爱情和葡萄酒一样，对程序员来说都是奢侈品”，这是学习“比喻”这种修辞手法时，微软小冰的一句话。现在，已经有4.5亿台第三方智能设备搭载了小冰，小冰多轮对话的轮数（CPS）最高已经达到了23轮。过去5年来，小冰团队有48篇论文发在了AAAI I JCAI ACL KDD EMNLP等顶会上，已经申请了72个专利，其中，今年发了3篇ACL、4篇EMNLP、1篇Interspeech和1篇ACM MM long paper。  
在前不久的一次workshop上，微软小冰首席科学家宋睿华、微软小冰首席NLP科学家武威、微软小冰首席语音科学家栾剑分享了近年来小冰的技术成就。  
- [CPU与GPU两者之间的区别与是什么？xPU又是什么 | 电子森林](https://mp.weixin.qq.com/s/RpLAHbfYDBF2M4khWMgTmw)  
摘要：先了解什么是异构并行计算，同构计算是使用相同类型指令集和体系架构的计算单元组成系统的计算方式。而异构计算主要是指使用不同类型指令集和体系架构的计算单元组成系统的计算方式，常见的计算单元类别包括CPU、GPU、DSP、ASIC、FPGA等。  
异构计算用简单的公式可以表示为“CPU+XXX”。举例来说，AMD着力发展的APU就属于异构计算，用公式表示就是CPU+GPU。  
由于术业有专攻，CPU、GPU、DSP、ASIC、FPGA各有所长，在一些场景下，引入特定计算单元，让计算系统变成混合结构，就能让CPU、GPU、DSP、FPGA执行自己最擅长的任务。  
- [TVM在CPU上加速卷积神经网络的策略与性能对比 | ApacheMXNet](https://mp.weixin.qq.com/s/cjPhpyS9TXtOHfRPs5pVPQ)  
摘要：无论是在云上还是在终端没有一款深度学习框架可以在各种主流CPU（包括Intel, AMD和ARM）都高效地进行卷积神经网络推理，主要原因是大家的做法都过于依赖第三方库，再加上框架本身不可避免地带来了一些overhead。为了做一个面向不同CPU，依赖度尽量少的高效模型推理方案，我们把目光投向了深度学习编译器。  
当前功能最全社区活跃度最高的开源深度学习编译器是TVM，它可以接受不同框架（Keras/MXNet/TensorFlow/...）的模型并编译到多种设备（CPU/GPU/FPGA/...）上。由于TVM的编译做了不同粒度的优化，在端到端（end-to-end）性能上有时甚至能得到几十倍的加速。  
- [ETHZ计算机系统结构课程 | 量子位](https://mp.weixin.qq.com/s/RoZj64VW-RFOSMSEpEpFqg)  
摘要：苏黎世联邦理工（ETH Zurich）今年的计算机系统结构的秋季课程在线开放。授课老师是ETH Zurich的Onur Mutlu教授。他是UT奥斯丁的博士，之前也是CMU的Strecker Professor。还曾经在Google、VMware、微软、Intel、AMD这些国际大厂工作过。校内，这门课被分成了10个星期的课程，目前已经更新了8个星期。   
课程链接：https://safari.ethz.ch/architecture/fall2019/doku.php?id=schedule

## [往期回顾](https://github.com/ysh329/awesome-embedded-ai)

- [2019-11-18](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-11-18.md)
- [2019-10-31](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-10-31.md)
- [2019-10-17](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-10-17.md)  
- [2019-10-03](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-10-03.md)  
- [2019-09-16](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-09-16.md)
- [2019-08-30](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-08-30.md)
- [2019-08-15](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-08-15.md)
- [2019-07-30](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-07-30.md)
- [2019-07-15](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-07-15.md)
- [2019-06-29](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-06-29.md)
- [2019-06-17](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-06-17.md)
- [2019-05-30](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-05-30.md)  
- [2019-05-15](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-05-15.md)  
- [2019-04-27](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-04-27.md)  
- [2019-04-13](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-04-13.md)  
- [2019-03-31](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-03-31.md)  

----

![wechat_qrcode](../wechat_qrcode.jpg)

Wechat ID: NeuroMem  
Editor: https://github.com/ysh329  
Project: https://github.com/ysh329/awesome-embedded-ai  

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 通用许可协议</a>进行许可。
