---
layout: default
---

# 嵌入式AI简报 (2020-08-26)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：本期19条。【新闻】美商务部再发声明欲断华为芯片后路，IBM首款7nm EUV商用处理器POWER10面世增加AI推理指令，首发于iPhone12的苹果A14与酷睿i9的性能对比，高通次旗舰骁龙860/875 Lite曝光；【论文】商汤等提出针对arm/gpu的2到8bit卷积的优化效果拔群，美东北大学提出模式化稀疏度权重剪枝并开发基于AI编译的手机端推理框架，速度快支持任意尺度的CNN模型结构OverNet，精度速度双超MobileNetV2的端侧模型结构ANTNet；【开源】特斯拉AI总监实现300行代码的minGPT，百度开源PaddleOCR 8.6MB OCR模型训练推理一并支持，多平台轻量级PyTorch模型推理框架MsnhNet，TensorFlow Lite增强在手机GPU OpenCL方面的性能；【博文】阿里PAI团队的AI编译优化工作在业务上的实践探索，armv7架构下3x3盒子滤波的优化实践，地平线提出或许比MLPerf更优的MAPS测评指标，使用NVIDIA安培架构inline PTX发挥GPU Tensor Core潜能，最后是最近MLPerf的结果解读之《AI训练芯片的巅峰对决》。

好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 华为：麒麟9000将采用台积电5nm工艺打造，CPU采用A77架构、GPU为Mali G77，性能相比上代进一步提升，同时拥有更强大的5G能力和AI处理能力；
- 华为：美商务部再发最新声明，欲断掉华为芯片后路：买第三方芯片也不行，华为云等38家子公司也进入“实体清单；
- 台积电：过去7个月已造出十亿颗7nm芯片。作为第一家将EUV投入7nm时代商业化生产的公司，今年能够批量生产5nm；
- 台积电：台积电第26届技术研讨会上公布3nm技术细节，明年试产，分享了5nm后续产品N5P和N4工艺节点进展。正建8000人2nm新研发中心；
- Imagination：RIOS国际开源实验室（RISC-V International OpenSource Laboratory）与Imagination Technologies建立战略伙伴关系，共同助力RISC-V生态发展；
- IBM：首款7nm EUV商用处理器POWER10面世，增加了对AI推理相关的指令和数据类型的支持，AI推理性能最高提升20倍；
- 英伟达：最快月底收购ARM，估值或达500亿美元，双方已进入排他性谈判阶段。


> 注：个别链接打不开，请点击文末【阅读原文】跳转

## 业界新闻


- [苹果A14有多强？竞品是英特尔酷睿i9 | 安兔兔](https://mp.weixin.qq.com/s/xamgu9JKh3n283jqGaYvKw)  
摘要：A14X Bionic芯片将首发于下一代iPad Pro产品上，而它的性能也将获得史无前例的大幅度提升，称其性能几乎可以与英特尔酷睿i9-9880H相当。  
今年9月的iPhone 12系列5G手机，预计会有四款，将采用A14 Bionic芯片，用上台积电5nm工艺，性能相比前代提升约50%，而苹果也宣布了将逐步从英特尔的芯片过渡到自研A系列芯片的消息，所以今后iPhone和iPad的对比对象都会是移动笔记本平台的芯片产品了。  
- [骁龙860/骁龙875 Lite曝光，真正的高通次旗舰5G芯片 | 安兔兔](https://mp.weixin.qq.com/s/knW2QFzDepYTiXjXGHn5NQ)  
摘要：有消息称，高通正规划骁龙800系列家族的次旗舰5G芯片，填补骁龙800系列、骁龙700系列之间的空白，更好应对华为麒麟800系列、天玑800系列等竞品。据说暂定名为骁龙860，即骁龙865精简，外挂5G基带。而下一代，有望年底推出新旗舰骁龙875G，集成5G基带，也可能有一精简版骁龙875 Lite。  
此前高通计划在今年第四季度商用骁龙662、骁龙460，明年第一季度商用骁龙875G、骁龙435G，明年第二季度商用骁龙735G。其中，骁龙875G、骁龙735G采用三星最新的8nm工艺制造，但据称该工艺目前良品率还不达标，可能会影响高通的计划。  
- [英特尔甩出六大新技术雪耻！两款GPU已在路上 | 芯东西](https://mp.weixin.qq.com/s/WqDm4wfbYYzjTd0Wujtm1w)  
摘要：8月14日英特尔在2020年架构日上推出10nm SuperFin晶体管技术，用于英特尔代号为“Tiger Lake”的下一代移动处理器，也基于此，推出名为“Willow Cove”的CPU微架构，该架构在前代基础上实现超越代间CPU性能的提升，大幅增强频率和功率效率。同时，该架构在更大的非相容1.25MB MLC中，引入了新设计的缓存架构，并通过控制流强制技术（Control Flow Enforcement Technology）增强了安全性。  
此外，也推出了Xe图形架构，主要有Xe-LP（低功耗）、Xe-HP、Xe-HPC和Xe-HPG四个系列。Tiger Lake是英特尔第一个采用全新Xe-LP（低功耗）微架构的SoC架构，能够对CPU、AI加速器进行优化，实现CPU、AI和图形性能的进一步提升。  
其中，Xe-LP是英特尔针对PC和移动计算平台的最高效架构。它的最高配置EU单元为96组，并具有新架构设计，包括异步计算、视图实例化（view instancing）、采样器反馈（sampler feedback）、带有AV1的更新版媒体引擎，以及更新版显示引擎等。  
- [PC的CPU“大小核”将至，或将拉开新时代的序幕 | 三易生活](https://mp.weixin.qq.com/s/I_vrm_ZIKJfU335mGRa-EQ)  
摘要：PC CPU的“大小核”时代快要来了。  
根据Intel官方产品库（Intel ARK）公布的信息显示，Core i5-L16G7属于全新的“Lakefield”家族。其CPU部分采用了一颗Sunny Cove大核心和四颗Tremont小核心，主频为1.40GHz至3GHz。  
2020年8月初，一份关于Intel第12代酷睿处理器家族AlderLake详细规格的文档遭到披露，其中清楚的显示，AlderLake系列中除了极少数型号之外，绝大多数SKU的CPU部分均由“Golden Cove”新架构大核心与“Gracemont”新架构小核心共同组成“big·SAMLL”的大小核设计。其中既有8大核8小核的旗舰型号，也会有2大核8小核的入门款“十核”。  
AMD也在研究“大小核”CPU设计方案。其相关专利明确提到大小核间转移任务时，需要判断当前进程是否分别兼容大核与小核的指令集。而这则基本上明示了AMD的“大小核”设计理念会与Intel一样，采用架构、指令集、性能都差异较大的两种CPU核心设计高低搭配。  
控制成本、简化编程，是“大小核”背后的需求。虽然目前的初代“大小核”PC处理器说实在的性能或许还有些不济，但大家要看到，它们的功耗实际上已经只比当下功耗最高的手机SoC高不到1W了。随着未来PC CPU厂商继续技术深耕，保不齐哪天x86架构的“大小核”处理器在能效比上就真有可能压倒如今越来越臃肿的ARM移动芯片。   
- [iOS AR APP软件：让生活中的纸片人动起来，一周内成手机应用榜单第一名 | 机器之心](https://mp.weixin.qq.com/s/RvG0RpKCy8QiA5H7-939cA)  
摘要：一款名为「RakugakiAR」的APP正式发布一周之内，便登顶亚洲多地应用商店下载榜，在 twitter 等社交媒体上也已经开始刷屏。  
AR 也不是什么新鲜技术了，为什么这款这么火？首先，Rakugaki 在日语中是涂鸦的意思，顾名思义，它的独特之处在于：无论你画什么，画功有多鬼畜，它都能让画里的内容动起来，因为对摄像头模组有特殊要求，目前这款软件仅有 iOS 版。  


## 论文


- [ICPP 2020] [Extremely Low-bit Convolution Optimization for Quantized Neural Network on Modern Computer Architectures](https://jnamaral.github.io/icpp20/slides/Han_Extremely.pdf)    
Video：https://youtu.be/aC--JOyqRPs  
Slide：https://jnamaral.github.io/icpp20/slides/Han_Extremely.pdf  
摘要：随着神经网络模型规模增大，量化是较为常用的模型压缩方法之一。现代处理器如ARM CPU和nvidia gpu虽已提供了对低位算术指令的支持。但在极低位卷积计算上，如ARM处理器在2到8位和NVIDIA GPU如4位、8位上，仍然缺乏有效和实用的优化。  
本文探讨了在不同体系结构上实现极低位卷积的性能优化方法。在ARM处理器上，作者提出了2到3位，和4到8位卷积的两种指令方案以及相应的寄存器分配方法。此外也重新设计了GEMM计算的数据填充和包装优化。还实现了特定比特宽度（如4到6比特）的winograd卷积算法，以获得更高的性能。  
在nvidia gpu上，也提出了一种数据划分机制和多级内存访问优化，以更好地适应GPU线程和内存层次结构的计算。也提出量化融合以消除不必要的数据存取。  
实验结果表明，与ncnn和cuDNN等最先进的框架和库相比，我们实现了更好的极低位卷积性能。据作者所知，这是第一个提供极低位卷积的高效实现，包括ARM CPU上的2到8位和NVIDIA GPU上的4位/8位。针对arm/gpu的2到8bit卷积的优化，与cudnn/tensorRT/ncnn/tvm(2bit)对比速度提升明显。  
- [2001.07710] [美国东北大学等提出新型剪枝方案，基于编译优化的移动端AI框架：YOLOv4目标检测、换脸、视频上色全部实时手机端实现 | 极市平台](https://mp.weixin.qq.com/s/EGqim7gw3N8gt9HTIoGiAw)  
标题：An Image Enhancing Pattern-based Sparsity for Real-time Inference on Mobile Devices  
文章：https://arxiv.org/abs/2001.07710  
项目：https://www.cocopie.ai/  
B站：https://space.bilibili.com/573588276  
摘要：通过可视化 VGG-16 在 ImageNet 上的预训练模型的部分权重，发现（i）卷积核的有效面积（即具有较高绝对值的权重）形成一些特定形状并在模型中反复出现，（ii）某些卷积核的权重值非常小，因此并不能对输出产生有效的激活，作者认为这种卷积核是无效卷积核。基于上述两个发现，该研究提出了一个新的稀疏性维度——模式化稀疏度，并且提出了基于模式化稀疏度的深度神经网络权重模式化剪枝的概念。  
在算法层面，作者设计了模式化稀疏度感知训练框架（pattern-aware network pruning framework），能够同时实现卷积核模式集的自动提取，模式化稀疏度的自动选择与模型训练。在卷积核模式集的自动提取中，首先构建一个模式集全集，包含了所有可能种类的卷积核模式。在训练过程中，将这个模式集作为稀疏化目标，通过 ADMM（alternating direction method of multipliers）将原始剪枝问题解耦为 Primal-Proximal 问题，迭代式地通过传统梯度下降法求解 Primal 问题，并引入一个二次项迭代求解 Proximal 问题。通过每次 Primal-Proximal 迭代更新，使卷积核动态地从模式集中选择当前最优的卷积核模式，并同时通过梯度下降法训练该模式非零位置的权重。当卷积核对稀疏模式的选择趋于稳定的时候（一般仅需要迭代 3-5 次），就可以删除掉那些被选择次数非常少的卷积核模式，从而将模式集的大小降低，并用更新后的模式集进行下一轮迭代，最终实现模式集的自动提取。  
作者首先确定了每一个卷积核中应保留 4 个非零值，控制模式集总集的大小，同时也利于移动端 CPU/GPU 的 SIMD 结构，并在 ImageNet 图像上的推理速度与在现有的深度神经网络加速器（TVM、MNN、TensorFlow-Lite）上做了速度对比。  
- [2008.02382] [OverNet: 速度快&高性能&任意尺度超分 | AIWalker](https://mp.weixin.qq.com/s/sFWdZZrZY_5USF_bbyQZFA)  
标题：OverNet: Lightweight Multi-Scale Super-Resolution with Overscaling Network  
文章：https://arxiv.org/abs/2008.02382  
摘要：基于CNN的超分方法往往存在计算量过大的问题，同时大多模型仅能处理特定超分比例，进而导致泛化性能缺失，提升了内存占用需求(注：这里指的是模型部署过程中的模型大小)。  
为解决上述局限性，作者提出了OverNet，一种轻量型CNN网络用于单模型任意尺度图像超分。首先，作者引入一种轻量型递归特征提取器，它通过跳过链接、稠密连接进行特征的重复与有效应用；然而，为最大化特征提取器的性能，作者提出了一种高精度重建模块，它可以轻易嵌入到现有超分网络中并改进性能；最后，作者引入多尺度损失函数并获得了跨尺度泛化性能。  
作者通过实验验证了所提方法的优异性能，具有更少的参数量、更优的性能。该文的主要贡献包含以下几点：一种轻量型递归特征提取器；一种过尺度模块用于生成过尺度特征并进而用于生成任意尺度输出，它可以有效提升模型的重建效果；一种新颖的多尺度损失函数，它可以同时进行单模型多尺度训练。  
- [1904.03775] [ANTNet: 端侧架构，精度速度双超MobileNetV2 | AIWalker](https://mp.weixin.qq.com/s/fauHZ-kcYnYQdzwLnbI3Sg)  
标题：ANTNets: Mobile Convolutional Neural Networks for Resource Efficient Image Classification  
文章：https://arxiv.org/abs/1904.03775  
代码：https://github.com/yyxiongzju/ANTNets  
摘要：尽管深度分离卷积是一种比标准卷积更有效的计算单元，但它往往会导致模型的表达能力降低。基于资源负载约束(比如计算消耗和参数量)，作者提出一种新颖的模块ANTBlock。通过在ANTBlock中引入注意力机制(位于depthwise卷积与投影层之间，与MobileNetV3类似)，它可以在高维空间提升模型的表达能力。  
作者通过实验表明：基于ANTBlock构建的ANTNet可以跨数据集取得更优的性能(相比同等计算消耗的端侧网络架构，比如MobileNet与ShuffleNet系列)。在CIFAR100数据集上，所提方法取得了75.7%的top1精度，它比MobileNetV2高1.5%且少8.3%的参数量与19.6%的计算量；在ImageNet数据集上，所提方法取得了72.8%的top1精度，它比MobileNetV2高0.8%，同时在iphone5上的速度为157.7ms(比MobileNetV2快20%)。  
- [2008.04175v1] [EagerPy: Writing Code That Works Natively with PyTorch, TensorFlow, JAX, and NumPy](https://arxiv.org/abs/2008.04175v1)  
代码：https://github.com/jonasrauber/eagerpy  
摘要：EagerPy是一个Python实现的框架，无需编写对应框架的代码，便可自动使用PyTorch、TensorFlow、JAX和NumPy。换句话说，如果觉得这个库不好用，用户可以轻松切换到其它框架上。此外，EagerPy提供了完善的类型注释和不同框架同一方法的统一且一致的支持。


## 开源项目


> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。

- [karpathy/minGPT: 特斯拉AI总监Karpathy写了个GPT的Pytorch训练库，仅300行代码 | 机器之心](https://mp.weixin.qq.com/s/aPA0PEqVn509u3xbgmhIwQ)  
摘要：近日，特斯拉人工智能研究负责人、前 OpenAI 研究科学家 Andrej Karpathy 基于 PyTorch，仅用 300 行左右的代码就写出了一个小型 GPT 训练库，并将其命名为 minGPT。Karpathy 表示，这个 minGPT 能够进行加法运算和字符级的语言建模，而且准确率还不错。  
- [PaddlePaddle/PaddleOCR: 百度PaddleOCR再发大招：自研顶会SOTA算法正式开源 | 飞桨PaddlePaddle](https://mp.weixin.qq.com/s/H_etZDJjfx1t2FaIJ7gGdA)  
摘要：首先，简单对比一下目前主流OCR方向开源repo的核心能力：  
    - 对于语种方面，easyOCR的优势在于多语言支持，非常适合有小语种需求的开发者；
    - 从预训练模型来看，easyOCR目前暂无超轻量模型，Chineseocr_lite最新的模型是10M左右，而PaddleOCR提供的8.6M是目前业界已知最轻量的； 
    - 对于部署方面，easyOCR模型较大不适合端侧部署，Chineseocr_lite和PaddleOCR都具备端侧部署能力；  
    - 对于自定义训练，实际业务场景中，预训练模型往往不能满足需求，对于自定义训练和模型Finetuning，目前只有PaddleOCR支持。  
PaddleOCR 8.6M超轻量模型，支持自定义训练、丰富的部署方式（覆盖服务器端、移动端/嵌入式端（apk/sdk）多场景需求）。  
- [msnh2012/Msnhnet: 多平台轻量级PyTorch模型推理框架MsnhNet | GiantPandaCV](https://mp.weixin.qq.com/s/U47E4ZF8OPstCmgnIR7TuA)  
摘要：一款受到darknet启发、基于纯c++的轻量级推理框架，支持windows, linux, macOS系统，支持intel x86, amd x86(未测试)和arm。特点是支持Pytorch模型的部署。  
- [TensorFlow/TensorFlow：谷歌称 TensorFlow Lite的 OpenCL 将推理性能提高一倍 | TensorFlow](https://mp.weixin.qq.com/s/cEMM_FzjEzeByQvPHI_HuQ)  
摘要：新的 TensorFlow 推理引擎提供了一个优化器，可以选择合适的工作组规模来提高性能，从而在高通（Qualcomm）的 Adreno GPU 等硬件上比平均速度提高了 50%。它原生支持 FP16，并且需要加速器来指定数据类型的可用性，通过加速算法计算来减少内存和带宽的使用以及训练时间。（Google 指出，由于 FP16 的支持，某些较旧的 GPU，如大约 2012 年上市的 Adreno 305，现在可以以其全部能力运行了。）另外，OpenCL 能够通过物理常量内存保持协同，从而大大超过 OpenGL 的性能。物理常量内存是像 Adreno GPU 这样的芯片中的一项硬件功能，为存储常量数组和变量保留了 RAM。  
Google 指出，OpenCL 并非标准 Android 发行版的一部分，因此一些用户无法使用它。作为一种权宜之计，TensorFlow Lite 现在可以在运行时检查 OpenCL 的可用性，这样，如果它不可用或无法加载，那么库就会退回到旧的 OpenGL 后端。  

## 博文


- [阿里PAI团队在业务实践上的AI编译优化](https://mp.weixin.qq.com/s/YZzwHTkRQuse_HhuQthltA)  
摘要：在这篇文章里，我们会介绍一个AI编译优化技术在阿里内部实际业务场景的落地case。AI编译技术在业务落地的过程中，不仅完善了自身的完备性和优化效果，同时在和业务结合的过程中，加深了我们对于线上环境复杂度的认知，理解了分布式训练中带来的各种挑战和问题，在过程中也积累了一些获取最优模型迭代性能的best practice。  
- [AI基准测试MLPerf模型少、更新慢，地平线提出的MAPS会更好吗？ | CCF-GAIR 2020](https://mp.weixin.qq.com/s/GhoekBz_IU5UrcpmcvVUxw)  
摘要：地平线联合创始人兼技术副总裁黄畅指出：MLPerf有模型更新慢、模型少、模型选择受各种因素影响的挑战。他也首次提出了新的方法用以评估芯片的AI真实性能——MAPS (Mean Accuracy-guaranteed Processing Speed，在精度有保障范围内的平均处理速度)。  
MAPS是评估AI芯片真实性能更好的方法吗？AI芯片性能的评估需要快、准、省，在这三个维度下地平线提出的新的AI芯片性能评估的方法称为MAPS（Mean Accuracy-guaranteed processing speed），意思是在精度有保障的范围评测芯片的平均效能，得到一个全面、完整、客观、真实的评估。  
MAPS的计算公式：MAPS = 所围面积 /（最高精度-最低精度），含义为在 ImageNet 的主流精度范围（75%~80%）下，速度最快的模型所代表的点（由精度和帧率确定）所围多边形面积大小即为芯片处理ImageNet AI任务的能力大小。
其代表的真实的AI效能也有对应的公式：`MAPS/Watt & MAPS/＄=TOPS/ Watt & TOPS/＄ X Utilization X MAPS/TOPS`。这三个要素中，第一个TOPS/Watt、TOPS/$是传统的方式。中间的要素有效利用率，是根据架构特点，利用编译器等去统化地解决极其复杂的带约束的离散优化问题，得到一个算法在芯片上运行的实际的利用率，实际是软硬件计算架构的优化目标。第三个要素是AI算法效率，指的是每消耗一个TOPS算力，带来的实际AI算法性能，体现的是AI算法效率的持续提升。  
- [基于NCNN的3x3可分离卷积再思考盒子滤波 | GiantPandaCV](https://mp.weixin.qq.com/s/bfxbRtdviPuXM4MJc_AyAQ)  
摘要：这篇文章主要是对NCNN 的3x3可分离卷积的armv7架构的实现进行了非常详细的解析和理解，然后将其应用于盒子滤波，并获得了最近关于盒子滤波的优化实验的最快速度，希望对做工程部署或者算法优化的读者有一定启发。  
- [如何利用 NVIDIA 安培架构 GPU 的新一代 Tensor Core 对计算进行极致加速 | NVIDIA开发者社区](https://mp.weixin.qq.com/s/DE7HyoUUUNSiBXczkJLsAg)  
摘要：2020年5月14日，NVIDIA发布了最新的 GPU 架构:安培，以及基于安培架构最新的 GPU A100。在安培架构中新增了功能强大的第三代 Tensor Core 单元。相较于 V100，A100 上搭载的第三代 Tensor Core 增加了对 DL （Deep Learning）和 HPC （High Performance Computing） 数据类型的全面支持，提高了各精度的运算吞吐能力，同时新增了稀疏运算特性，进一步实现了峰值运算性能的翻倍。  
本文将会回顾一下 Tensor Core 的发展路线，并介绍如何通过 inline PTX 使用 Tensor Core如mma PTX 指令还有ldmatrix PTX 指令等等。  
- [AI训练芯片巅峰对决，如何正确“围观” | StarryHeavensAbove](https://mp.weixin.qq.com/s/iipdabuVjesHJ6w9OecgNg)  
摘要：AI芯片Benchmark MLPerf最近公开了训练测试0.7版本的结果。这次的结果有以下几个主要看点和亮点：
新硬件亮相：Google TPU4应该算第一次公开亮相，Nvidia A10，华为的Ascend910第一次参与。此外，这次Nvidia的A100和Google TPUv4系统，CPU则来自AMD。
训练系统的规模继续飙升：Google TPU v3系统最多4096个处理器，TPU v4系统最多256个处理器；Nvidia V100系统最多1536个处理器，A100系统最多2048个处理器；中国科学院深圳先进技术研究院提交的Ascend910系统最多1024个处理器。
自有框架和硬件相结合：Framework方面，Nvidia针对推荐系统的Merlin/HugeCTR，Google的JAX和华为的Mindspore都是第一次亮相。这几个框架和Tensorflow，Pytorch相比都针对自家硬件有更多性能的优化。
