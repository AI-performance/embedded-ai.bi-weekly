---
layout: default
---

# 嵌入式AI简报 (2019-10-31)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  
<font>注：PC端微信链接打不开请用手机打开</font>


## 业界新闻  

- [ARM发布的新IP:Ethos N57和N37 NPU、Mali-G57 GPU以及Mali-D37 DPU | 电脑爱好者](https://mp.weixin.qq.com/s/C-fhsIt5vq6iCGbtxEurJA)  
摘要：ARM此次发布的新IP包括Ethos N57和N37 NPU、Mali-G57 GPU以及Mali-D37 DPU。  
Mali-G57采用了和Mali-G77相同的Valhall架构，改进了图形指令集、运算架构等。与Mali-G52相比，G57有着1.3倍的性能，能效提升30%、性能密度提升30%、机器学习提升60%。换句话说，在搭配相同计算核心的情况下（如Mali-G57MP4 PK Mali-G76MP4），Mali-G57的性能可以超越Mali-G76（现在的情况是Mali-G52MP6才略强于Mali-G76MP4）！  
“Ethos”是ARM自研的NPU架构，此次新上市的Ethos N57主要用于主流中端手机合智慧家庭，Ethos N3则适用于入门手机合电视等，它们的性能分别为2TOP/s和1TOP/s。作为对比，ARM在今年5月推出的旗舰级Ethos N77 NPU则拥有4TOP/s的性能。嗯，未来的中端SoC将具备更好的原生AI性能。  
最后我们再来看看Mali-D37 DPU（深度学习运算单元），它基于Komeda架构设计，使用16工艺时只需占用1平方毫米的面积就能输出2K视频。考虑到2020将7nm将成为主流，届时这个视觉核心IP将释放更大的潜力。  
- [三星发布990旗舰处理器：7nm EUV，自研M5架构，G77，LPDDR5 | EETOP](https://mp.weixin.qq.com/s/E_jj9ijLKg2bFg9CWSjHnw)  
摘要：三星在9月初的时候刚刚发布了其中端处理器Exynos 980，在今天，三星又出乎意料地宣布推出全新的Exynos 990芯片组-这应该是三星2020年的旗舰SoC，它采用最新IP并采用了最新的7nm EUV制造工艺。与前期发布的Exynos 980一样，这两款新的芯片都带支持5G连接，并采用了新的产品命名方案。  
之前有报道说三星已放弃自研架构，不过此次发布的Exynos 990依然采用了三星的自研的Mongoose架构 ，并且已迭代到了最新的Exynos M5微体系结构。三星引述说，新设计比9820中的Exynos M4快20％-考虑到更大的工艺节点改进，20%的提升相对有点保守。  
三星已经从上一代的Cortex A75升级了中间核，使其具有更新的Cortex A76设计，这将在日常工作负载中提供更大的性能提升。最后，我们继续将Cortex A55内核视为低效率内核。三星没有透露任何CPU的时钟频率，但宣称新的三群集的总体改进为13％。  
Exynos 990看到了以MP11配置的新型Mali-G77的形式对GPU进行的重大升级。新的Valhall架构有望为Arm的GPU IP带来巨大的性能提升，而且看起来新的Exynos将成为采用新GPU系列的首批芯片组之一。三星新闻稿称，我们将看到性能提升高达20％，或者功耗效率提升高达20％。考虑到工艺改进以及新的GPU架构，这种增长显得微不足道，三星似乎不太可能赶上苹果和高通的最新处理器。  
Exynos 990自身带来的一项根本变化是从LPDDR4X内存到LPDDR5内存的过渡。这样一来，新芯片组就可以将内存控制器频率从2093 MHz提高到2750MHz，同时提高了功耗效率。  
三星透露，该芯片带有双核NPU和新的DSP，NPU吞吐量超过10个TOP。在这里，尚不清楚三星是指DSP的功能还是NPU和DSP的组合功能。Exynos 9820的NPU的吞吐量为1.86TOP。  
- [平头哥共享MCU芯片平台，成为国内第一家芯片平台开源企业 | 量子位](https://mp.weixin.qq.com/s/AjiKV31KlJSJhL995nmv6g)  
摘要：平头哥宣布开源其低功耗微控制芯片（MCU）设计平台，成为国内第一家推进芯片平台开源的企业。平台面向AIoT时代的定制化芯片设计需求，目标群体包括芯片设计公司、IP供应商、高校及科研院所等。  
全世界的开发者都能基于该平台设计面向细分领域的定制化芯片，IP供应商可以研发原生于该平台的核心IP，高校和科研院所则可开展芯片相关的教学及科研活动。平台包含处理器、基础接口IP、操作系统、软件驱动和开发工具等模块，搭载基于RISC-V架构的玄铁902处理器，提供多种IP以及驱动，能让用户快速集成、快速验证，减少基础模块开发成本。  
平头哥还透露，后续还将开放更多IP和玄铁处理器。平台开源代码包括基础硬件代码和配套软件代码两部分，现已公布在GitHub开源社区。  
- [科大讯飞的1024：语音技术进一步突破，发布专用芯片 | 机器之心](https://mp.weixin.qq.com/s/hxmB95Zv3vOheOvrH9FceQ)  
摘要：在 AI 生态产品发布环节，科大讯飞集团副总裁、消费者 BG 副总裁于继栋正式发布了联合生态合作伙伴打造的家电行业专用语音芯片 CSK400X 系列。  
于继栋表示，AIoT 已经成为科大讯飞的核心战略之一。在 AIoT 时代，智能硬件市场对芯片的算力提出了更高的需求，但从芯片市场的现状来看，高算力与价格往往难以取得平衡，而且芯片与算法的适配难度比较高，MCU 也常常并非针对神经网络而设计。  
科大讯飞与生态合作伙伴穹天科技根据讯飞 AI 算法为 CSK400X 系列语音芯片设计了 NPU 框架与规格，并设计了针对神经网络的底层算子。据了解，这款芯片可借助神经网络算法解决家居中的噪音问题，算力可达到 128GOPS，同时支持全栈语音能力，包括离线唤醒、远场阵列降噪、回声消除等优化功能，适用智能家电、玩具、音箱、离线门禁等落地场景。   
- [功耗仅2W、算力达4Tops，地平线发布旭日二代边缘 AI 芯片 | 雷锋网](https://mp.weixin.qq.com/s/H2ZO9WxkBlKzjX-cElZdYg)  
摘要：地平线正式发布旭日二代边缘 AI 芯片及一站式全场景芯片解决方案。  
旭日二代是地平线面向未来物联网推出的新一代智能应用加速引擎，也是地平线在自动驾驶芯片领域技术先发优势的一次成功迁移。在旭日二代上的实际测试结果表明，分类模型 MobileNet V2的运行速度超过每秒700张图片，检测模型Yolo V3的运行速度超过每秒40张图片。在运行这些业界领先的高效模型方面，旭日二代能够达到甚至超过业内标称4TOPS算力的AI芯片，而其功耗仅为2W。  
- [华为发布5G全系列解决方案: 行业首个ADN Mobile解决方案 | 华为无线网络](https://mp.weixin.qq.com/s/25ZKJshAy6EQbZ3t_cszdA)  
摘要：华为无线总裁邓泰华发布了行业第一个ADN Mobile（自动驾驶移动网络）解决方案。该解决方案包含AI训练平台iMaster NAIE、跨域AI单元 iMaster AUTIN和MBB网络AI单元iMaster MAE三部分，这三个部分通过分层自治形成最小闭环，并按需垂直协同。基于该解决方案，华为将在明年推出系列化的无线网络自动驾驶的L3应用。

## 论文

- [GPU解码提升40倍，英伟达推进边缘设备部署语音识别，代码已开源 | 机器之心](https://mp.weixin.qq.com/s/6b-cmb8iVhYk50BpMYsNyQ)  
摘要：英伟达近日一篇论文为语音识别技术在边缘设备上的部署带来了福音，其新提出的解码器方法即使在边缘嵌入式 GPU 上也能高效高速地执行。而且这种方法不仅适用于低端硬件，而且也能为数据中心带来显著的效率提升，从而能够识别更多并行的在线音频流。该方法的早期版本已开源。  
总体而言，相比于单核 CPU 解码，新提出的改进能实现高达 240 倍的提速，并且解码速度也比当前最佳的 GPU 解码器快 40 倍，同时返回的结果表现相当。从大型数据中心服务器到低功耗边缘设备，该架构可在各种层级的硬件上部署生产级模型。  
论文：https://arxiv.org/pdf/1910.10032.pdf  
代码：https://github.com/kaldi-asr/kaldi/tree/master/src/cudadecoder  
- [第三方应用AI BenchMark: 麒麟990远超骁龙855+，华为手机领跑前五 | 机器之心](https://mp.weixin.qq.com/s/ChbfJSj509-se_yxwkGP7w)  
摘要：过去两年，移动 AI 加速器的性能一直在快速提升，每出现一代新的系统级芯片（Soc），性能就会提升近两倍。当前的第四代移动 NPU 性能已经接近不久前英伟达推出的 CUDA 兼容显卡性能，并且加之以移动深度学习框架性能的提升，第四代 NPU 甚至可以在移动设备端上运行复杂和深度 AI 模型。  
在本文中，来自苏黎世联邦理工学院、谷歌研究院和华为、高通、三星、联发科、紫光展锐等多家移动端芯片厂商的研究者评估并对比了高通、海思、三星、联发科和紫光展锐为 AI 推理提供硬件加速的芯片组的性能结果。此外，他们还探讨了安卓 ML pipeline 近来的变化，概述了深度学习模型在移动端设备上的部署情况。  
- [ICCV 2019 开源论文 | 基于元学习和AutoML的模型压缩新方法 | PaperWeekly](https://mp.weixin.qq.com/s/lc7IoOV6S2Uz5xi7cPQUqg)  
摘要：模型剪枝算法能够减少模型计算量，实现模型压缩和加速的目的，但是模型剪枝过程中确定剪枝比例等参数的过程实在让人头痛。  
这篇文章提出了 PruningNet 的概念，自动为剪枝后的模型生成权重，从而绕过了费时的 retrain 步骤。并且能够和进化算法等搜索方法结合，通过搜索编码 network 的 coding vector，自动地根据所给约束搜索剪枝后的网络结构。和 AutoML 技术相比，这种方法并不是从头搜索，而是从已有的大模型出发，从而缩小了搜索空间，节省了搜索算力和时间。  


## 开源项目

- [nfrechette/rtm: Realtime Math](https://github.com/nfrechette/rtm)  
摘要：This library is geared towards realtime applications that require their math to be as fast as possible. Much care was taken to maximize inlining opportunities and for code generation to be optimal when a function isn't inlined by passing values in registers whenever possible.  
- [Kyson/AndroidGodEye: AndroidGodEye:A performance monitor tool , like "Android Studio profiler" for Android , you can easily monitor the performance of your app real time in pc browser](https://github.com/Kyson/AndroidGodEye)  
摘要：Android开发者在性能检测方面的工具一直比较匮乏，仅有的一些工具，比如Android Device Monitor，使用起来也有些繁琐，对开发者能力有一定的要求。而线上的App监控更无从谈起。所以需要有一个系统能够提供Debug和Release阶段全方位的监控，更深入地了解对App运行时的状态。    
AndroidGodEye是一个可以在PC浏览器中实时监控Android性能数据指标的工具，你可以通过wifi/usb连接手机和pc，通过pc浏览器实时监控手机性能。系统分为三部分：  
  1. Core 核心部分，提供所有模块  
  2. Debug Monitor部分，提供Debug阶段开发者面板  
  3. Toolbox 快速接入工具集，给开发者提供各种便捷接入的工具  
AndroidGodEye提供了多种监控模块，比如cpu、内存、卡顿、内存泄漏等等，并且提供了Debug阶段的Monitor看板实时展示这些数据。而且提供了api供开发者在release阶段进行数据上报。  


## 博文

- [移动端arm cpu优化学习笔记第2弹--常量阶时间复杂度中值滤波 | 知乎](https://zhuanlan.zhihu.com/p/87516875)  
摘要：最近在复现 Side window 中值滤波的时候就在思考中值滤波能怎么优化，直观上看中值滤波好像没什么可优化的点，因为中值滤波需要涉及到排序，而且半径越大，排序的耗时也越大。那么中值滤波能否进一步加速呢？或者像均值滤波一样，可以不受滤波半径的影响呢？  
答案是能！这篇博客就是记录怎么去优化中值滤波的实践过程。而前面的3小节都是介绍尝试的优化思路，最后一节才是讲本文标题提到的常量阶时间复杂度中值滤波的实现思路，想直接看其实现思路的读者可以跳到最后一小节。  
- [神经架构搜索研究指南，只看这一篇就够了 | InfoQ](https://www.infoq.cn/article/JgQPbhS7Irx9dlYMuxTu)  
摘要：从训练到用不同的参数做实验，设计神经网络的过程是劳力密集型的，非常具有挑战性，而且常常很麻烦。但是想象一下，如果能够将这个过程实现自动化呢？将这种想象转变为现实，就是本指南的核心内容。  
我们将探索一系列的研究论文，这些论文试图解决具有挑战性的自动化神经网络设计任务。在本指南中，我们假设读者尝试过使用 Keras 或 TensorFlow 等框架从头开始设计神经网络。  
- [PyTorch&TensorFlow跑分对决：哪个平台运行NLP模型推理更快 | 量子位](https://mp.weixin.qq.com/s/V_RAEbGLjZUq3UjGW52eEQ)  
摘要：作者在PyTorch 1.3.0、TenserFlow2.0上分别对CPU和GPU的推理性能进行了测试。  
两种不同的环境中具体硬件配置如下：CPU推理：使用谷歌云平台上的n1-standard-32硬件，即32个vCPU、120GB内存，CPU型号为2.3GHz的英特尔至强处理器。  
GPU推理：使用谷歌云平台上的定制化硬件，包含12个vCPU、40GB内存和单个V100 GPU（16GB显存）。  
除了初步的测试，作者还用上两个平台独有的加速工具，看看它们对模型推理速度有多大的提升。  
Pytorch使用TorchScript可以在XLNet上产生永久的性能提升，而在XLM上使用则会不可靠；在XLM上，TorchScript可以提高较小输入时的性能，但会降低较大输入时的性能。  
TF启用XLA提高了速度和内存使用率，所有模型的性能都有提高。  大多数基准测试的运行速度提升到原来的1.15倍。在某些极端情况下，推理时间减少了70％，尤其是在输入较小的情况下。  
- [独家深度 | 为什么Arm的AI处理器姗姗来迟 | 雷锋网](https://mp.weixin.qq.com/s/Z9-W46L9O_OHRgzL4XkzxQ)  
摘要：上周，Arm推出了一系列全新的IP，包括NPU、GPU以及DPU。NPU尤为值得关注，不仅因为NPU系列同时发布了N57和N37两款新品，还因为Arm的ML处理器（Machine Learning Processor）系列名称Ethos也正式公布。全新AI系列产品的亮相，意味着Arm的AI策略更加明晰。  
不过，2017年开始，手机市场就开启了AI处理器的竞争，华为、苹果、三星、联发科、高通都相继推出集成NPU的手机处理器。为什么Arm直到2019年才推出NPU？Arm的NPU能否获得成功？
