---
layout: default
---

# 嵌入式AI简报 (2020-01-27)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：本次内容20条，关于BERT有4条。「业界新闻」搜狗的手机AR实景导航挺有意思，其中「论文」三篇BERT量化/加速方面的文章，「开源项目」基于端MNN的上层SDK MNNKit、浏览器端BLAS库值得关注，「博文」关于兼顾速度精度与工程实现的CNN结构设计的技巧值得一看，以及TFLITE端侧基于部分层训练的迁移学习实战！

## 业界新闻

- iPhone 12将采用5纳米A14处理器 | IT之家  
摘要：1月17日，据国外媒体报道， iPhone 12将采用5纳米制造工艺的A14处理器，配备ToF 3D摄像头（AR能力和人像模式显著提升）和6GB RAM，支持5G网络。5纳米工艺意味着A14芯片可能拥有125亿个晶体管，甚至比桌面和服务器CPU更多。此外，苹果还可以将芯片面积缩小至约85平方毫米，这将使其性能大幅提升，尤其是在多核性能方面。  
- [Qualcomm推出三款全新骁龙移动平台460，662，720以满足对4G智能手机的持续需求 | Qualcomm中国](https://mp.weixin.qq.com/s/2BO_UXIY1qGoqLlp5tdYMQ)  
摘要：因为比较关注CPU/GPU/DSP部分，简单摘录了一下：  
  - 骁龙720G支持最新的第五代Qualcomm® AI Engine，集成了增强的Qualcomm® Hexagon™张量加速器。  
  - 骁龙460是面向新一代大众市场智能手机的移动平台，在骁龙4系中实现了性能的巨大飞跃，同时在连接、AI和拍摄等方面也有显著的提升。骁龙460首次在骁龙4系中引入CPU性能内核以及新的GPU架构，分别实现了70%和60%的性能提升。整体系统性能是前代平台的2倍。骁龙460还首次在骁龙4系中引入了支持HVX的Hexagon处理器，因此通过对第三代Qualcomm AI Engine的支持再加上Qualcomm传感器中枢，可为拍摄和语音助理带来全新的AI体验。  
  - 骁龙662首次将令人惊叹的拍摄和AI功能带入骁龙6系。支持Hexagon向量扩展内核（HVX）的第三代Qualcomm AI Engine与Qualcomm Spectra 340T ISP一起。  
- [搜狗地图：国内首个手机 AR 实景驾驶导航上线，还能识别车辆行人 | 量子位](https://mp.weixin.qq.com/s/3WBQtHzqI4oaDfrDYHKiPg)  
摘要：搜狗地图支持手机 AR 实景行车驾驶导航，能贴合道路给出路径指引，转弯处还有立体箭头引导，能够识别车辆、行人，及时给出碰撞预警，夜间识别，也不在话下。无需额外购买昂贵的设备，只需一台手机，一个App，即可体验。  
- [谷歌 GMS 服务替代方案；华为移动核心服务——HMS Core | 机器之心](https://mp.weixin.qq.com/s/oJWWYEbw7JsZ7jplOcNTIw)  
摘要：去年 5 月以来，在美国政府的「禁令」之下，谷歌宣布停止与华为合作，尤其是不再提供谷歌移动服务（GMS）。为打破谷歌封锁，华为一手推动鸿蒙系统问世，一手发力自有华为移动核心服务——HMS Core，取代谷歌 GMS 服务。  
HMS Core 4.0新开放的机器学习服务（ML Kit）。它为开发者提供文字识别、人脸识别、标签识别、对象检测和跟踪、地标识别等 AI 能力。以地标识别为例，该数据集中包含了 300 万张图片，囊括全球 3 万处独特地表，量级是普通数据集的 30 倍，基于华为云强大的计算能力和高效的地标识别算法，应用软件可以瞬间精准识别地标，为用户提供智能化的体验。  

## 论文

- [AdaBERT：推理速度提升29倍，参数少1/10，阿里提出 BERT 压缩方法 | 机器之心](https://mp.weixin.qq.com/s/mObuD4ijUCjnebYIrjvVdw)  
论文：AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search  
链接：https://arxiv.org/pdf/2001.04246v1.pdf  
摘要：阿里巴巴的压缩方案 AdaBERT，该方案特点是能针对具体任务得到性能不会显著下降的小型模型。现有的方法是将 BERT 压缩成小型模型，但这种压缩方法与任务无关，也就是说对于不同的下游任务而言，压缩方法是一样的。面向任务的 BERT 压缩方法是有必要的而且很有用，为此阿里巴巴的研究者提出了一种全新的压缩方法 AdaBERT。该方法利用了可微神经架构搜索来自动将 BERT 压缩成适应不同特定任务的小型模型。  
研究者为 AdaBERT 提出了两种不同的损失函数。一是面向任务的知识注入损失，可为搜索过程提供提示；二是效率感知型损失，这能提供搜索约束。这两个损失能为任务适应型 BERT 压缩提供效率和有效性之间的平衡。在多个 NLP 任务上对 AdaBERT 进行了评估，结果表明这些任务适应型压缩模型在保证表现相当的同时，推理速度比 BERT 快 12.7 到 29.3 倍，同时参数缩小至 BERT 的 11.5 到 17.0 之一的规模。  
- [AAAI 2020] [Q-BERT：超低精度量化BERT，UC伯克利提出用二阶信息压缩神经网络 | 机器之心](https://mp.weixin.qq.com/s/0qBlnsUqI2I-h-pFSgcQig)  
论文：Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT  
地址：https://arxiv.org/pdf/1909.05840.pdf   
摘要：加州大学伯克利分校 Zhewei Yao 博士分享了他的 AAAI 论文。该研究介绍了一种使用二阶信息进行模型压缩的新型系统性方法，能够在图像分类、目标检测和自然语言处理等一系列具有挑战性的任务中产生前所未有的小模型，最小化性能下降幅度，同时保持硬件效率。为此，研究者使用了多项新技术，并提出了新模型 Q-BERT。对二阶信息（即 Hessian 信息）进行大量逐层分析，进而对 BERT 执行混合精度量化。研究发现，与计算机视觉领域中的神经网络相比，BERT 的 Hessian 行为存在极大的不同。因此，该研究提出一种基于 top 特征值均值和方差的敏感度度量指标，以实现更好的混合精度量化。这与仅利用均值的研究相反。  
提出新的量化机制——组量化（group-wise quantization），该方法能够缓解准确率下降问题，同时不会导致硬件复杂度显著上升。具体而言，组量化机制将每个矩阵分割为不同的组，每个组拥有独立的量化范围和查找表。调查了 BERT 量化中的瓶颈，即不同因素（量化机制，以及嵌入、自注意力和全连接层等模块）如何影响 NLP 性能和模型压缩率之间的权衡。  
- [LTD-BERT：内存用量1/20，速度加快80倍，腾讯QQ提出全新BERT蒸馏框架，未来将开源 | 机器之心](https://mp.weixin.qq.com/s/W668zeWuNsBKV23cVR0zZQ)  
摘要：BERT 的运算速度和资源开销是很难权衡的问题。GPU 上线速度较快，但是成本很高；CPU 上线的话运算速度较慢，需要做大量的底层优化工作。为此，QQ 团队提出一种基于知识蒸馏的方法。基于从 BERT 得到的 sentence embedding 去完成更上层任务的需求，这也能满足当前对于 BERT 的大部分的需求，囊括了文本分类、文本聚类、相似度计算等等。当然，word-level 的压缩也可以以近似的方法去实现。  
从 2019 年 8 月份在腾讯内部开源至今，LTD-BERT 的效果已经在 QQ、腾讯新闻、腾讯游戏、腾讯看点、腾讯健康等服务海量用户的产品的实际任务中得到验证，确保算法拥有足够的泛化能力和实用性。此外，该团队还表示 LTD-BERT 相关代码和更多结果将在近期开源。  
- [MeliusNet：第一次胜过MobileNet的二值神经网络，-1与+1的三年艰苦跋涉 | 机器之心](https://mp.weixin.qq.com/s/Xvlxs-Os2meduHrEQFc7vg)  
论文：MeliusNet: Can Binary Neural Networks Achieve MobileNet-level Accuracy?  
地址：https://arxiv.org/pdf/2001.05936v1.pdf  
摘要：来自德国波茨坦大学的 Joseph Bethge 和 Haojin Yang 等研究者提出了 MeliusNet ，其准确度上能击败之前所有二值模型，甚至超越了 MobileNetV1。MeliusNet 的计算复杂度不高，能充分利用二值网络的速度优势。整体而言，MeliusNet 继续在 BNN 定制化架构上进行探索，为二值网络设计了一套高效简洁的架构。它主要由 Dense Block 与 Improvement Block 组成。其中 Dense Block 主要用于扩充特征的表达能力，而 Improvement Block 主要用于提升特征的质量。  
- [AAAI 2020] [UVA-Net：速度提升200倍，爱奇艺&北航等提出基于耦合知识蒸馏的视频显著区域检测算法 | 机器之心](https://mp.weixin.qq.com/s/VbvCTEYC2FSMAff6bkjAjQ)  
论文：Ultrafast Video Attention Prediction with Coupled Knowledge Distillation  
地址：https://arxiv.org/pdf/1904.04449.pdf  
摘要：作者设计了一个超轻量级网络 UVA-Net，并提出了一种基于耦合知识蒸馏的网络训练方法，在视频注意力预测方向的性能可与 11 个最新模型相媲美，而其存储空间仅占用 0.68 MB，在 GPU，CPU 上的FPS分别达到 10、106、404，比之前的模型提升了 206 倍。  
- [AdderNets：深度学习可以不要乘法，北大、华为诺亚新论文：加法替代，效果不变，延迟大降 | 机器之心](https://mp.weixin.qq.com/s/bki5Axgct2-RFIQWLumGcA)  
论文：AdderNet: Do We Really Need Multiplications in Deep Learning?  
地址：https://arxiv.org/pdf/1912.13200v2.pdf  
摘要：北大、华为诺亚方舟实验室等的研究者提出了一个为减少计算量的网络：AdderNets，用于将深度神经网络中特别是卷积中的乘法，转换为加法运算。  
在 AdderNets 中，研究者采用了 L1 正则距离，用于计算滤波器和输入特征之间的距离，并作为输出的反馈。为了取得更好的性能，研究者构建了一种特殊的反向传播方法，并发现这种几乎完全采用加法的神经网络能够有效收敛，速度与精度优秀。  
从结果来看，AdderNets 在 ResNet-50 上 对 ImageNet 数据集进行训练后，能够取得 74.9% 的 top-1 精确度和 91.7% 的 top-5 精确度，而且在卷积层上不使用任何乘法操作。  

## 开源项目

- [阿里开源MNNKit：基于MNN的移动端深度学习SDK，支持安卓和iOS | 机器之心](https://mp.weixin.qq.com/s/ylfoNHnZRailjvb3tDdfSQ)  
摘要：近日，阿里开源了基于 MNN 引擎的项目 MNNKit，面向安卓和 iOS，以 SDK 的方式提供 AI 端侧推理能力，不能把模型从里面掏出来用。Kit的逻辑相比 MNN 比较上层。开发者不需要了解算法细节就可以直接使用。  
目前，MNNKit 已经有人脸检测、手势识别、人像分割等，后续可能有更多 API 接入。  
据悉，MNNKit 是 MNN 团队在阿里系应用大规模业务实践后的成熟解决方案，历经双十一等项目考验，在不依赖于后端的情况下进行高性能推理，使用起来稳定方便。  
项目地址：https://github.com/alibaba/MNNKit  
- [waylonflinn/weblas: GPU Powered BLAS for Browsers](https://github.com/waylonflinn/weblas)  
摘要：GPU accelerated Javascript. Numerical computing in your browser with performance comparable to native. Currently includes hundreds of unit tests, which verify correctness on hundreds of millions of data points.   
Our focus is on numerical operations useful for neural networks and machine learning. So far, we've got 32-bit versions of each of these:  
sscal - Matrix (and Vector) Scale (with addition)  
sgemm - Matrix Multiply  
sdwns - Matrix (and Image) Downsample (for Max Pooling)  
sclmp - Matrix clamp (for ReLU)  
项目地址：https://github.com/waylonflinn/weblas   
- [brendangregg/FlameGraph: Stack trace visualizer](https://github.com/brendangregg/FlameGraph)  
摘要：Flame graphs are a visualization of profiled software, allowing the most frequent code-paths to be identified quickly and accurately. They can be generated using my open source programs on github.com/brendangregg/FlameGraph, which create interactive SVGs.  
文档：http://www.brendangregg.com/flamegraphs.html  
项目：https://github.com/brendangregg/FlameGraph   
- [Facebook开源低延迟在线自动语音识别框架：速度更快，错误率更低 | AI前线](https://mp.weixin.qq.com/s/TjSxiRxCx7A-lz_KorUFrw)  
摘要：Facebook 人工智能研究院（FAIR）开源了基于深度学习的推理框架 wav2letter @ anywhere，该框架可在云或嵌入式边缘环境中快速实现在线自动语音识别。  
Wav2letter @ anywhere 是由 wav2letter 和 wav2letter ++ 这两个基于神经网络的语言模型构建的，在 2018 年 12 月发布时，Facebook 人工智能研究院认为这两款语言模型是目前可用的最快的开源语音识别系统。  
wav2letter 项目地址：
https://github.com/facebookresearch/wav2letter  
- [微软开源ONNX Runtime模型以加速Google BERT | AI前线](https://mp.weixin.qq.com/s/EHYmsSc9OqHcK_54Dw2d4w)  
摘要：微软人工智能研究院 1 月 21 日称计划开源 BERT 自然语言模型优化版本，该模型可以与 ONNX Runtime 推理引擎配合使用。在为 Bing 搜索引擎提供语言表达功能时，Microsoft 使用相同的模型来降低 BERT 的延迟。该模型“为 Bing 用户带来了最佳搜索体验” ，2019年秋天发表的一篇论文中对该模型进行了详细介绍。  
公司发言人表示，这意味着开发人员可以使用 ONNX Runtime 和 Nvidia V100 GPU 大规模部署 BERT，而延迟只有 1.7 毫秒，这样的性能表现过去只能在大型科技公司中实现。  
微软在其他自然语言开发上也取得了一定进展。在 2019 年温哥华 NeurIPS 上，微软和浙江大学联合发布了语音合成系统 FastSpeech，与自回归的 Transformer TTS 相比，FastSpeech 将梅尔谱的生成速度提高了近 270 倍，将端到端语音合成速度提高了 38 倍，单 GPU 上的语音合成速度达到了实时语音速度的 30 倍。  
原文地址：https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/  


## 博文

- [CNN结构设计技巧-兼顾速度精度与工程实现 | 知乎](https://zhuanlan.zhihu.com/p/100609339)  
摘要：CNN结构设计技巧，本文从分割、low-level、检测、metric learning、分类、landmark、视频理解、3D等角度对CNN的结构设计进行了解读，非常值得一看。  
- [详谈ARM架构与ARM内核发展史 | CSDN云计算](https://mp.weixin.qq.com/s/vhdWstSIeqnCv2fK7abhJA)  
摘要：目前为止，ARM总共发布8种架构：ARMv1、ARMv2、ARMv3、ARMv4、ARMv5、ARMv6、ARMv7 、ARMv8，这是ARM架构指令集的多个v版本。  
基于不同的ARM架构可以设计出不同特点的内核处理器。比如基于ARMv3架构设计出的处理器ARM6、ARM7，这两款处理器适用于不同的场景，硬件可能不同，但是架构指令集是一样的。  
- [5nm工艺问世，CPU工艺与性能是一种什么样的关系 | strongerHuang](https://mp.weixin.qq.com/s/4z-4Fd17BT7tnVe5iLNTBw)  
摘要：现在半导体工艺上所说的多少nm工艺其实是指线宽，也就是芯片上的最基本功能单位门电路的宽度，因为实际上门电路之间连线的宽度同门电路的宽度相同，所以线宽可以描述制造工艺。缩小线宽意味着晶体管可以做得更小、更密集，而且在相同的芯片复杂程度下可使用更小的晶圆，于是成本降低了。  
更先进半导体制造工艺另一个重要优点就是可以提升工作频率，缩减元件之间的间距之后，晶体管之间的电容也会降低，晶体管的开关频率也得以提升，从而整个芯片的工作频率就上去了。  
- [深度学习加速：算法、编译器、体系结构与硬件设计 | 知乎](https://zhuanlan.zhihu.com/p/101544149)  
摘要：NeurlPS2019 大会的「Efficient Processing of Deep Neural Network: from Algorithms to Hardware Architectures」的演讲概括性地介绍了目前深度学习加速领域的进展，这个演讲的逻辑清晰，结合演讲ppt内容和近期调研的一些加速器相关内容，总括性地理一下深度学习加速领域的内容。首先关于深度学习加速，一般会想到的就是关于深度学习加速器的硬件设计，但其实更宽泛地讲，从算法顶层，到编译器，到体系结构，硬件最底层都有涉及。下面的介绍也大致围绕这几个方面展开。  
- [用 TensorFlow Lite 实现设备端个性化模型 | TensorFlow](https://mp.weixin.qq.com/s/4tBJplpdhceBw6z_vO3Cag)  
摘要：虽然 TensorFlow Lite 训练解决方案仍在开发中，但已有设备端上座迁移学习的例子了。本文会向您介绍可现学现用的设备端机器学习模型个性化方法、使用场景，以及这背后的工作原理。  
现有的例子有一个 Android 应用可学习对相机图像进行实时分类，使用迁移学习技术，这款应用能够在所有适用的最新 Android 设备（系统版本为 5.0 及以上）上运行。在此应用中，我们可以针对不同目标类别拍摄样本照片，然后在设备端对其进行训练。
该应用会对 MobileNetV2 量化模型在端上使用迁移学习技术，而我们先前已（在服务器上）基于 ImageNet 对该模型进行了预训练，并将最后几层替换成可训练的 softmax 分类器。可以通过训练最后几层来识别四个任意的新类别，而准确率则取决于要捕获的类别的“难度”。我们观察到，即使只有数十个样本，也足以取得良好的结果。
